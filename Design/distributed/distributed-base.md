---
title: 分布式基本原理
categories: ['design', 'distributed']
tags: ['design', 'distributed']
date: 2018-07-09 17:43
---

# 分布式基本原理

## 简介

### 性能

常见的性能指标：

- **`吞吐量(Throughput)` - 是指系统在单位时间内处理请求的数量**。
- **`响应时间(Response Time, RT)` - 是指系统对请求作出响应的时间**。
- **并发用户数** - 是指系统可以同时承载的正常工作的在线用户数。
- **`每秒查询率（QPS）`** - 是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准。在因特网上，作为域名系统服务器的机器的性能经常用每秒查询率来衡量。对应 fetches/sec，即每秒的响应请求数，也即是最大吞吐能力。
- **每秒事务数（Transaction Per Second, TPS）** - 衡量系统处理事务或交易的能力，即服务器对客户请求的能力，每秒处理的事务数，一般在 LoadRunner 上使用，设置事务，然后统计单位时间内系统可以成功完成多少个定义的事务。
- **点击量（Hits Per Second）** - 服务器每秒收到的 HTTP 请求数。

```
QPS = 并发量 / 平均响应时间
```

吞吐量和响应时间这两个指标往往是矛盾的，追求高吞吐的系统，往往很难做到低响应时间，解释如下：

- 在无并发的系统中，吞吐量为响应时间的倒数，例如响应时间为 10 ms，那么吞吐量为 100 req/s，因此高吞吐也就意味着低响应时间。
- 但是在并发的系统中，由于一个请求在调用 I/O 资源的时候，需要进行等待。服务器端一般使用的是异步等待方式，即等待的请求被阻塞之后不需要一直占用 CPU 资源。这种方式能大大提高 CPU 资源的利用率，例如上面的例子中，单个请求在无并发的系统中响应时间为 10 ms，如果在并发的系统中，那么吞吐量将大于 100 req/s。因此为了追求高吞吐量，通常会提高并发程度。但是并发程度的增加，会导致请求的平均响应时间也增加，因为请求不能马上被处理，需要和其它请求一起进行并发处理，响应时间自然就会增高。

### 可用性

可用性指系统在面对各种异常时可以提供正常服务的能力。可以用系统可用时间占总时间的比值来衡量，4 个 9 的可用性表示系统 99.99% 的时间是可用的。

重要术语：

- **异常**
  - **服务器宕机** - 内存错误、服务器停电等都会导致服务器宕机，此时节点无法正常工作，称为不可用。服务器宕机会导致节点失去所有内存信息，因此需要将内存信息保存到持久化介质上。
  - **网络异常** - 有一种特殊的网络异常称为——**网络分区** ，即集群的所有节点被划分为多个区域，每个区域内部可以通信，但是区域之间无法通信。
  - **磁盘故障** - 磁盘故障是一种发生概率很高的异常。使用冗余机制，将数据存储到多台服务器。
- **超时** - 在分布式系统中，一个请求除了成功和失败两种状态，还存在着超时状态。可以将服务器的操作设计为具有 **幂等性** ，即执行多次的结果与执行一次的结果相同。如果使用这种方式，当出现超时的时候，可以不断地重新请求直到成功。

### 一致性

可以从两个角度理解一致性：从客户端的角度，读写操作是否满足某种特性；从服务器的角度，多个数据副本之间是否一致。

### 可扩展性

指系统通过扩展集群服务器规模来提高性能的能力。理想的分布式系统需要实现“线性可扩展”，即随着集群规模的增加，系统的整体性能也会线性增加。

## 数据分布

分布式存储系统的数据分布在多个节点中，常用的数据分布方式有哈希分布和顺序分布。

数据库的水平切分（Sharding）也是一种分布式存储方法，下面的数据分布方法同样适用于 Sharding。

### 哈希分布

哈希分布就是将数据计算哈希值之后，按照哈希值分配到不同的节点上。例如有 N 个节点，数据的主键为 key，则将该数据分配的节点序号为：hash(key)%N。

传统的哈希分布算法存在一个问题：当节点数量变化时，也就是 N 值变化，那么几乎所有的数据都需要重新分布，将导致大量的数据迁移。

**一致性哈希**

Distributed Hash Table（DHT）：对于哈希空间 [0, 2<sup>n</sup>-1]，将该哈希空间看成一个哈希环，将每个节点都配置到哈希环上。每个数据对象通过哈希取模得到哈希值之后，存放到哈希环中顺时针方向第一个大于等于该哈希值的节点上。

一致性哈希的优点是在增加或者删除节点时只会影响到哈希环中相邻的节点，例如下图中新增节点 X，只需要将数据对象 C 重新存放到节点 X 上即可，对于节点 A、B、D 都没有影响。

### 顺序分布

哈希分布式破坏了数据的有序性，顺序分布则不会。

顺序分布的数据划分为多个连续的部分，按数据的 ID 或者时间分布到不同节点上。例如下图中，User 表的 ID 范围为 1 \~ 7000，使用顺序分布可以将其划分成多个子表，对应的主键范围为 1 \~ 1000，1001 \~ 2000，...，6001 \~ 7000。

顺序分布的优点是可以充分利用每个节点的空间，而哈希分布很难控制一个节点存储多少数据。

但是顺序分布需要使用一个映射表来存储数据到节点的映射，这个映射表通常使用单独的节点来存储。当数据量非常大时，映射表也随着变大，那么一个节点就可能无法存放下整个映射表。并且单个节点维护着整个映射表的开销很大，查找速度也会变慢。为了解决以上问题，引入了一个中间层，也就是 Meta 表，从而分担映射表的维护工作。

### 负载均衡

衡量负载的因素很多，如 CPU、内存、磁盘等资源使用情况、读写请求数等。

分布式系统存储应当能够自动负载均衡，当某个节点的负载较高，将它的部分数据迁移到其它节点。

每个集群都有一个总控节点，其它节点为工作节点，由总控节点根据全局负载信息进行整体调度，工作节点定时发送心跳包（Heartbeat）将节点负载相关的信息发送给总控节点。

一个新上线的工作节点，由于其负载较低，如果不加控制，总控节点会将大量数据同时迁移到该节点上，造成该节点一段时间内无法工作。因此负载均衡操作需要平滑进行，新加入的节点需要较长的一段时间来达到比较均衡的状态。

## 分布式系统理论

### CAP

**分布式系统不可能同时满足一致性（C：Consistency）、可用性（A：Availability）和分区容忍性（P：Partition Tolerance），最多只能同时满足其中两项。**

<div align="center">
<img src="http://dunwu.test.upcdn.net/cs/java/javaweb/distributed/architecture/分布式理论-CAP.jpg!zp" width="450"/>
</div>

**（一）一致性**

**一致性指的是多个数据副本是否能保持一致的特性。**

- 在一致性的条件下，系统在执行数据更新操作之后能够从一致性状态转移到另一个一致性状态。
- 对系统的一个数据更新成功之后，如果所有用户都能够读取到最新的值，该系统就被认为具有强一致性。

**（二）可用性**

**可用性指分布式系统在面对各种异常时可以提供正常服务的能力，可以用系统可用时间占总时间的比值来衡量**，如：4 个 9 的可用性表示系统 `99.99%` 的时间是可用的。

在可用性条件下，系统提供的服务一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果。

**（三）分区容忍性**

**网络分区指分布式系统中的节点被划分为多个区域，每个区域内部可以通信，但是区域之间无法通信。**

在分区容忍性条件下，分布式系统在遇到任何网络分区故障的时候，仍然需要能对外提供一致性和可用性的服务，除非是整个网络环境都发生了故障。

**（四）权衡**

在分布式系统中，分区容忍性必不可少，因为需要总是假设网络是不可靠的。因此，CAP 理论实际在是要在可用性和一致性之间做权衡。

可用性和一致性往往是冲突的，很难都使它们同时满足。在多个节点之间进行数据同步时，

- 为了保证一致性（CP），就需要让所有节点下线成为不可用的状态，等待同步完成；
- 为了保证可用性（AP），在同步过程中允许读取所有节点的数据，但是数据可能不一致。

### BASE

**BASE 是基本可用（Basically Available）、软状态（Soft State）和最终一致性（Eventually Consistent）三个短语的缩写。**

BASE 理论是对 CAP 中一致性和可用性权衡的结果，它的理论的核心思想是：即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。

<div align="center">
<img src="http://dunwu.test.upcdn.net/cs/java/javaweb/distributed/architecture/分布式理论-BASE.png!zp" />
</div>

- **基本可用** - **指分布式系统在出现故障的时候，保证核心可用，允许损失部分可用性**。例如，电商在做促销时，为了保证购物系统的稳定性，部分消费者可能会被引导到一个降级的页面。
- **软状态** - 指允许系统中的数据存在中间状态，并认为该中间状态不会影响系统整体可用性。即**允许系统不同节点的数据副本之间进行同步的过程存在延时**。
- **最终一致性** - **最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能达到一致的状态**。

ACID 要求强一致性，通常运用在传统的数据库系统上。而 BASE 要求最终一致性，通过牺牲强一致性来达到可用性，通常运用在大型分布式系统中。

在实际的分布式场景中，不同业务单元和组件对一致性的要求是不同的，因此 ACID 和 BASE 往往会结合在一起使用。

## 共识性问题

### Paxos

用于达成共识性问题，即对多个节点产生的值，该算法能保证只选出唯一一个值。

主要有三类节点：

- 提议者（Proposer）：提议一个值；
- 接受者（Acceptor）：对每个提议进行投票；
- 告知者（Learner）：被告知投票的结果，不参与投票过程。

算法需要满足 safety 和 liveness 两方面的约束要求（实际上这两个基础属性是大部分分布式算法都该考虑的）：

- safety：保证决议结果是对的，无歧义的，不会出现错误情况。
  - 决议（value）只有在被 proposers 提出的 proposal 才能被最终批准；
  - 在一次执行实例中，只批准（chosen）一个最终决议，意味着多数接受（accept）的结果能成为决议；
- liveness：保证决议过程能在有限时间内完成。
  - 决议总会产生，并且 learners 能获得被批准（chosen）的决议。

基本过程包括 proposer 提出提案，先争取大多数 acceptor 的支持，超过一半支持时，则发送结案结果给所有人进行确认。一个潜在的问题是 proposer 在此过程中出现故障，可以通过超时机制来解决。极为凑巧的情况下，每次新的一轮提案的 proposer 都恰好故障，系统则永远无法达成一致（概率很小）。

Paxos 能保证在超过 $1/2$ 的正常节点存在时，系统能达成共识。

#### 单个提案者+多接收者

如果系统中限定只有某个特定节点是提案者，那么一致性肯定能达成（只有一个方案，要么达成，要么失败）。提案者只要收到了来自多数接收者的投票，即可认为通过，因为系统中不存在其他的提案。

但一旦提案者故障，则系统无法工作。

#### 多个提案者+单个接收者

限定某个节点作为接收者。这种情况下，共识也很容易达成，接收者收到多个提案，选第一个提案作为决议，拒绝掉后续的提案即可。

缺陷也是容易发生单点故障，包括接收者故障或首个提案者节点故障。

以上两种情形其实类似主从模式，虽然不那么可靠，但因为原理简单而被广泛采用。

当提案者和接收者都推广到多个的情形，会出现一些挑战。

#### 多个提案者+多个接收者

既然限定单提案者或单接收者都会出现故障，那么就得允许出现多个提案者和多个接收者。问题一下子变得复杂了。

一种情况是同一时间片段（如一个提案周期）内只有一个提案者，这时可以退化到单提案者的情形。需要设计一种机制来保障提案者的正确产生，例如按照时间、序列、或者大家猜拳（出一个数字来比较）之类。考虑到分布式系统要处理的工作量很大，这个过程要尽量高效，满足这一条件的机制非常难设计。

另一种情况是允许同一时间片段内可以出现多个提案者。那同一个节点可能收到多份提案，怎么对他们进行区分呢？这个时候采用只接受第一个提案而拒绝后续提案的方法也不适用。很自然的，提案需要带上不同的序号。节点需要根据提案序号来判断接受哪个。比如接受其中序号较大（往往意味着是接受新提出的，因为旧提案者故障概率更大）的提案。

如何为提案分配序号呢？一种可能方案是每个节点的提案数字区间彼此隔离开，互相不冲突。为了满足递增的需求可以配合用时间戳作为前缀字段。

此外，提案者即便收到了多数接收者的投票，也不敢说就一定通过。因为在此过程中系统可能还有其它的提案。

### Raft

Raft 算法是 Paxos 算法的一种简化实现。

包括三种角色：leader、candidate 和 follower，其基本过程为：

- **Leader 选举** - 每个 candidate 随机经过一定时间都会提出选举方案，最近阶段中得票最多者被选为 leader；
- **同步 log** - leader 会找到系统中 log 最新的记录，并强制所有的 follower 来刷新到这个记录；

_注：此处 log 并非是指日志消息，而是各种事件的发生记录。_

#### 单个 Candidate 的竞选

有三种节点：Follower、Candidate 和 Leader。Leader 会周期性的发送心跳包给 Follower。每个 Follower 都设置了一个随机的竞选超时时间，一般为 150ms\~300ms，如果在这个时间内没有收到 Leader 的心跳包，就会变成 Candidate，进入竞选阶段。

- 下图表示一个分布式系统的最初阶段，此时只有 Follower，没有 Leader。Follower A 等待一个随机的竞选超时时间之后，没收到 Leader 发来的心跳包，因此进入竞选阶段。

<div align="center">
<img src="http://dunwu.test.upcdn.net/cs/java/javaweb/distributed/architecture/raft-candidate-01.gif!zp" />
</div>

- 此时 A 发送投票请求给其它所有节点。

<div align="center">
<img src="http://dunwu.test.upcdn.net/cs/java/javaweb/distributed/architecture/raft-candidate-02.gif!zp" />
</div>

- 其它节点会对请求进行回复，如果超过一半的节点回复了，那么该 Candidate 就会变成 Leader。

<div align="center">
<img src="http://dunwu.test.upcdn.net/cs/java/javaweb/distributed/architecture/raft-candidate-03.gif!zp" />
</div>

- 之后 Leader 会周期性地发送心跳包给 Follower，Follower 接收到心跳包，会重新开始计时。

<div align="center">
<img src="http://dunwu.test.upcdn.net/cs/java/javaweb/distributed/architecture/raft-candidate-04.gif!zp" />
</div>

#### 多个 Candidate 竞选

- 如果有多个 Follower 成为 Candidate，并且所获得票数相同，那么就需要重新开始投票，例如下图中 Candidate B 和 Candidate D 都获得两票，因此需要重新开始投票。

<div align="center">
<img src="http://dunwu.test.upcdn.net/cs/java/javaweb/distributed/architecture/raft-multi-candidate-01.gif!zp" />
</div>

- 当重新开始投票时，由于每个节点设置的随机竞选超时时间不同，因此能下一次再次出现多个 Candidate 并获得同样票数的概率很低。

<div align="center">
<img src="http://dunwu.test.upcdn.net/cs/java/javaweb/distributed/architecture/raft-multi-candidate-02.gif!zp" />
</div>

#### 同步日志

- 来自客户端的修改都会被传入 Leader。注意该修改还未被提交，只是写入日志中。

<div align="center">
<img src="http://dunwu.test.upcdn.net/cs/java/javaweb/distributed/architecture/raft-sync-log-01.gif!zp" />
</div>

- Leader 会把修改复制到所有 Follower。

<div align="center">
<img src="http://dunwu.test.upcdn.net/cs/java/javaweb/distributed/architecture/raft-sync-log-02.gif!zp" />
</div>

- Leader 会等待大多数的 Follower 也进行了修改，然后才将修改提交。

<div align="center">
<img src="http://dunwu.test.upcdn.net/cs/java/javaweb/distributed/architecture/raft-sync-log-03.gif!zp" />
</div>

- 此时 Leader 会通知的所有 Follower 让它们也提交修改，此时所有节点的值达成一致。

<div align="center">
<img src="http://dunwu.test.upcdn.net/cs/java/javaweb/distributed/architecture/raft-sync-log-04.gif!zp" />
</div>

## 参考资料

- 杨传辉. 大规模分布式存储系统: 原理解析与架构实战[M]. 机械工业出版社, 2013.
- [区块链技术指南](https://www.gitbook.com/book/yeasy/blockchain_guide/details)
- [NEAT ALGORITHMS - PAXOS](http://harry.me/blog/2014/12/27/neat-algorithms-paxos/)
- [Raft: Understandable Distributed Consensus](http://thesecretlivesofdata.com/raft)
- [Paxos By Example](https://angus.nyc/2012/paxos-by-example/)
